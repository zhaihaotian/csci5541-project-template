<!DOCTYPE html>
<!-- saved from url=(0014)about:internet -->
<html lang=" en-US"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>NLP Class Project | Fall 2024 CSCI 5541 | University of Minnesota</title>

  <link rel="stylesheet" href="./files/bulma.min.css" />

  <link rel="stylesheet" href="./files/styles.css">
  <link rel="preconnect" href="https://fonts.gstatic.com/">
  <link href="./files/css2" rel="stylesheet">
  <link href="./files/css" rel="stylesheet">


  <base href="." target="_blank"></head>


<body>
  <div>
    <div class="wrapper">
      <h1 style="font-family: &#39;Lato&#39;, sans-serif;">Don’t Guess: Abstention Recognition and Clarification in Post-Training of LLMs</h1>
      <h4 style="font-family: &#39;Lato&#39;, sans-serif; ">Fall 2024 CSCI 5541 NLP: Class Project - University of Minnesota</h4>
      <h4 style="font-family: &#39;Lato&#39;, sans-serif; ">Hallucination Group</h4>

      <div class="authors-wrapper">
        
        <div class="author-container">
          <div class="author-image">
                        
              <img src="" alt="Haotian Zhai">
            
            
          </div>
          <p>
                        
              <strong>Haotian Zhai</strong><br/>
              Project lead & RLVR design
            
          </p>
        </div>
        
        <div class="author-container">
          <div class="author-image">
            
            <img src="" alt="Jingcheng Liang">
            
          </div>
          <p>
            
            <strong>Jingcheng Liang</strong><br/>
            Algorithm training & evaluation
            
          </p>
        </div>
        
        <div class="author-container">
          <div class="author-image">
            
              <img src="" alt="Haotian Huang">            
           
          </div>
          <p>
              <strong>Haotian Huang</strong><br/>
              Data cleaning & baselines
          </p>
        </div>
        
        <div class="author-container">
          <div class="author-image">
                        
              <img src="" alt="Zekang Li">
            
          </div>
          <p>
            <strong>Zekang Li</strong><br/>
            Data curation & evaluation
          </p>
        </div>
        
      </div>

      <br/>

      <div class="authors-wrapper">
        <div class="publication-links">
          <!-- Github link -->
          <span class="link-block">
            <a
              href="#"
              target="_blank"
              class="external-link button is-normal is-rounded is-dark is-outlined"
            >
            <span>Final Report (coming soon)</span>
            </a>
          </span>
          <span class="link-block">
            <a
              href="#"
              target="_blank"
              class="external-link button is-normal is-rounded is-dark is-outlined"
            >
            <span>Code (coming soon)</span>
            </a>
          </span>      
          <span class="link-block">
            <a
              href="#"
              target="_blank"
              class="external-link button is-normal is-rounded is-dark is-outlined"
            >
            <span>Model Weights (coming soon)</span>
            </a>
          </span>              
        </div>
      </div>


    </div>
  </div>





  
  


  <div class="wrapper">
    <hr>
    
    <h2 id="abstract">Abstract</h2>

<p>Large language models tuned with reinforcement learning and verifiable rewards are incentivized to always answer, even when the prompt is underspecified or outside of the model’s knowledge boundary, which amplifies hallucinations. We propose <em>Don’t Guess</em>, a post-training recipe that mixes answerable and unanswerable queries, rewards explicit abstention via <code>I don't know.</code>, and requires a concise clarification or explanation that is checked by an LLM-as-judge. By aligning incentives around calibrated refusals and actionable follow-up prompts, we aim to reduce hallucination rate, improve trustworthiness, and lower wasted computation on ill-posed questions.</p>

<hr>

<h2 id="teaser">Teaser Figure</h2>


<p class="sys-img"><img src="./files/example.png" alt="Contrasting ambiguous vs. unanswerable prompts"></p>


<h3 id="the-timeline-and-the-highlights">Ambiguity vs. Unanswerability</h3>

<p>Ambiguity stems from incomplete semantics (e.g., “Send <em>it</em> to Alex”), while unanswerability stems from complete semantics that nonetheless lack sufficient constraints (e.g., a rectangle with perimeter 20 but no side lengths). Both should trigger interaction, but only the latter merits a firm abstention.</p>

<hr>

<h2 id="introduction">Introduction / Background / Motivation</h2>

<p>
<b>What did you try to do? What problem did you try to solve? Articulate your objectives using absolutely no jargon.</b>
</p>
<p>
We want large language models to say “I don’t know” when a question is impossible to answer safely, and to follow up with a short request that explains what information is missing. The objective is a tutor-like agent that knows when to stop guessing and instead guides the user toward a resolvable version of the task.
</p>

<p>
<b>How is it done today, and what are the limits of current practice?</b>
</p>
<p>
Modern instruction tuning and RL with verifiable rewards (RLVR) reward models for delivering an answer, so the easiest way to get a high reward is to always respond—even when the prompt is underspecified, outside the model’s knowledge, or built on a false premise. Recent studies (Kalai et&nbsp;al., 2025; Kirichenko et&nbsp;al., 2025; Yao et&nbsp;al., 2025) show this causes more hallucinations and worse calibration than instruction-only baselines.
</p>

<p>
<b>Who cares? If you are successful, what difference will it make?</b>
</p>
<p>
Reliable abstention improves the usability of reasoning agents, reduces wasted tokens on fabricated answers, and keeps downstream decision makers from acting on incorrect information. Well-calibrated clarifications are also easier to audit, which benefits safety reviewers and researchers working on high-stakes domains.
</p>

<hr>

<h2 id="approach">Approach</h2>

<p>
<b>What did you do exactly? How did you solve the problem? Why did you think it would be successful? Is anything new in your approach?</b>
</p>

<p>
We curate paired answerable and unanswerable queries by filtering AbstentionBench and related datasets (Song et&nbsp;al., 2025; Wang et&nbsp;al., 2025; Zhang et&nbsp;al., 2025) and then auto-annotate each unanswerable sample with the clarifying question or explanation it should elicit. The RLVR objective mixes both query types: answerable items receive a rule-based reward for correctness, while unanswerable items only reward the policy when it both outputs <code>I don't know.</code> and produces the expected clarification as judged by a specialist LLM. This simple, verifiable contract targets calibrated behavior without supervising intermediate reasoning steps.
</p>

<p>
<b>What problems did you anticipate? What problems did you encounter? Did the very first thing you tried work?</b>
</p>

<p>
Judge reliability is the main risk—we rely on an LLM-as-judge to confirm whether a clarification matches the reference, and judge drift could destabilize RL. As a fallback MVP, we can restrict the domain to math problems (à la SUM) and replace free-form clarifications with a five-way classification over predefined unanswerability sources, ensuring deterministic reward signals even if RL training stalls.
</p>

<hr>
    
<h2 id="results">Results</h2>
<p>
<b>How did you measure success? What experiments were used? What were the results, both quantitative and qualitative? Did you succeed? Did you fail? Why?</b>
</p>
<p>
Full-scale RL runs are scheduled on an 8×A100-80GB MSI HPC node. Before that, we will prototype instruction prompts on a single A40/L40 and measure (1) abstention accuracy and over-refusal rate on held-out AbstentionBench subsets, (2) clarification helpfulness judged by Omni-Math style LLM evaluators, and (3) token savings from early termination. Success means matching baseline accuracy on answerable items while halving hallucination rate on unanswerable ones; failure modes will likely come from noisy judge signals or insufficient coverage of unanswerable patterns.
</p>
<table>
  <thead>
    <tr>
      <th style="text-align: center"><strong>Experiment</strong></th>
      <th style="text-align: center">1</th>
      <th style="text-align: center">2</th>
      <th style="text-align: center">3</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>Goal</strong></td>
      <td style="text-align: center">Abstention accuracy on curated AbstentionBench split</td>
      <td style="text-align: center">Clarification quality judged by LLM-as-judge</td>
      <td style="text-align: center">Fallback math-only MVP</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>Primary Metric</strong></td>
      <td style="text-align: center">Precision/recall of “I don’t know”</td>
      <td style="text-align: center">Judge agreement &amp; helpfulness scores</td>
      <td style="text-align: center">Classification accuracy over five unanswerable sources</td>
    </tr>
  </tbody>
  <caption>Table 1. Planned evaluation axes before and after RL fine-tuning.</caption>
</table>
<br>
<div style="text-align: center;">
<img style="height: 300px;" alt="" src="./files/results.png">
</div>
<br><br>

<hr>



<h2 id="conclusion">Conclustion and Future Work</h2>
<p>

Reproducibility hinges on transparent reward definitions and public data. We plan to release the filtered AbstentionBench subset plus auto-generated clarifications so other groups can rerun policy gradient training with PPO or GRPO. The main societal risk is over-abstention in critical scenarios, so we will audit for unnecessary refusals and document domains where the model under-performs. Future work includes broader coverage of modalities (vision, tools), integrating retrieval to shrink the “unknown” region, and stress-testing how safely the model escalates when faced with harmful or policy-breaking prompts.</p>


<hr>


  </div>
  


</body></html>
