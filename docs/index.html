<!DOCTYPE html>
<!-- saved from url=(0014)about:internet -->
<html lang=" en-US"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>NLP Class Project | Fall 2024 CSCI 5541 | University of Minnesota</title>

  <link rel="stylesheet" href="./files/bulma.min.css" />

  <link rel="stylesheet" href="./files/styles.css">
  <link rel="preconnect" href="https://fonts.gstatic.com/">
  <link href="./files/css2" rel="stylesheet">
  <link href="./files/css" rel="stylesheet">


  <base href="." target="_blank"></head>


<body>
  <div>
    <div class="wrapper">
      <h1 style="font-family: &#39;Lato&#39;, sans-serif;">Don’t Guess: Abstention Recognition and Clarification in Post-Training of LLMs</h1>
      <h4 style="font-family: &#39;Lato&#39;, sans-serif; ">Fall 2024 CSCI 5541 NLP: Class Project - University of Minnesota</h4>
      <h4 style="font-family: &#39;Lato&#39;, sans-serif; ">Hallucination Group</h4>

      <div class="authors-wrapper">
        
        <div class="author-container">
          <div class="author-image">
                        
              <img src="" alt="Haotian Zhai">
            
            
          </div>
          <p>
                        
              <strong>Haotian Zhai</strong><br/>
              Project lead & RLVR design
            
          </p>
        </div>
        
        <div class="author-container">
          <div class="author-image">
            
            <img src="" alt="Jingcheng Liang">
            
          </div>
          <p>
            
            <strong>Jingcheng Liang</strong><br/>
            Algorithm training & evaluation
            
          </p>
        </div>
        
        <div class="author-container">
          <div class="author-image">
            
              <img src="" alt="Haotian Huang">            
           
          </div>
          <p>
              <strong>Haotian Huang</strong><br/>
              Data cleaning & baselines
          </p>
        </div>
        
        <div class="author-container">
          <div class="author-image">
                        
              <img src="" alt="Zekang Li">
            
          </div>
          <p>
            <strong>Zekang Li</strong><br/>
            Data curation & evaluation
          </p>
        </div>
        
      </div>

      <br/>

      <div class="authors-wrapper">
        <div class="publication-links">
          <!-- Github link -->
          <span class="link-block">
            <a
              href="#"
              target="_blank"
              class="external-link button is-normal is-rounded is-dark is-outlined"
            >
            <span>Final Report (coming soon)</span>
            </a>
          </span>
          <span class="link-block">
            <a
              href="#"
              target="_blank"
              class="external-link button is-normal is-rounded is-dark is-outlined"
            >
            <span>Code (coming soon)</span>
            </a>
          </span>      
          <span class="link-block">
            <a
              href="#"
              target="_blank"
              class="external-link button is-normal is-rounded is-dark is-outlined"
            >
            <span>Model Weights (coming soon)</span>
            </a>
          </span>              
        </div>
      </div>


    </div>
  </div>





  
  


  <div class="wrapper">
    <hr>
    
    <h2 id="abstract">Abstract</h2>

<p>Large language models tuned with reinforcement learning and verifiable rewards are incentivized to always answer, even when the prompt is underspecified or outside of the model’s knowledge boundary, which amplifies hallucinations. We propose <em>Don’t Guess</em>, a post-training recipe that mixes answerable and unanswerable queries, rewards explicit abstention via <code>I don't know.</code>, and requires a concise clarification or explanation that is checked by an LLM-as-judge. By aligning incentives around calibrated refusals and actionable follow-up prompts, we aim to reduce hallucination rate, improve trustworthiness, and lower wasted computation on ill-posed questions.</p>

<hr>

<h2 id="teaser">Teaser Figure</h2>


<p class="sys-img"><img src="./files/example.png" alt="Contrasting ambiguous vs. unanswerable prompts"></p>


<h3 id="the-timeline-and-the-highlights">Ambiguity vs. Unanswerability</h3>

<p>Ambiguity stems from incomplete semantics (e.g., “Send <em>it</em> to Alex”), while unanswerability stems from complete semantics that nonetheless lack sufficient constraints (e.g., a rectangle with perimeter 20 but no side lengths). Both should trigger interaction, but only the latter merits a firm abstention.</p>

<hr>

<h2 id="introduction">Introduction / Background / Motivation</h2>

<p>
<b>What did you try to do? What problem did you try to solve? Articulate your objectives using absolutely no jargon.</b>
</p>
<p>
We want large language models to say “I don’t know” when a question is impossible to answer safely, and to follow up with a short request that explains what information is missing. The objective is a tutor-like agent that knows when to stop guessing and instead guides the user toward a resolvable version of the task.
</p>

<p>
<b>How is it done today, and what are the limits of current practice?</b>
</p>
<p>
Modern instruction tuning and RL with verifiable rewards (RLVR) reward models for delivering an answer, so the easiest way to get a high reward is to always respond even when the prompt is underspecified, outside the model’s knowledge, or built on a false premise. Recent studies (Kalai et&nbsp;al., 2025; Kirichenko et&nbsp;al., 2025; Yao et&nbsp;al., 2025) show this causes more hallucinations and worse calibration than instruction-only baselines.
</p>

<p>
<b>Who cares? If you are successful, what difference will it make?</b>
</p>
<p>
Reliable abstention improves the usability of reasoning agents, reduces wasted tokens on fabricated answers, and keeps downstream decision makers from acting on incorrect information. Well-calibrated clarifications are also easier to audit, which benefits safety reviewers and researchers working on high-stakes domains.
</p>

<hr>

<h2 id="approach">Approach</h2>

<p>
<b>What did you do exactly? How did you solve the problem? Why did you think it would be successful? Is anything new in your approach?</b>
</p>

<p>
We curate paired answerable and unanswerable queries by filtering AbstentionBench and related datasets (Song et&nbsp;al., 2025; Wang et&nbsp;al., 2025; Zhang et&nbsp;al., 2025) and then auto-annotate each unanswerable sample with the clarifying question or explanation it should elicit. The RLVR objective mixes both query types: answerable items receive a rule-based reward for correctness, while unanswerable items only reward the policy when it both outputs <code>I don't know.</code> and produces the expected clarification as judged by a specialist LLM. This simple, verifiable contract targets calibrated behavior without supervising intermediate reasoning steps.
</p>

<hr>

<h2 id="mid-check">Midterm Progress Check</h2>

<h3>1. Project Progress & Preliminary Results</h3>

<h4>1.1 Data Construction Progress</h4>
<p><strong>Research gap:</strong> No public dataset simultaneously covers our four essentials (paired answerable/unanswerable queries, explicit refusals, explanation of the missing information, and awareness of knowledge boundaries), so we curated new cross-domain data to fill that hole.</p>
<p><strong>Cross-domain test set:</strong> We down-sampled AbstentionBench, cleaning roughly 100 samples per domain with an o3-mini-assisted pipeline, and ended up with ~1,800 QA pairs dedicated to evaluating boundary awareness.</p>
<p><strong>Training set:</strong> We rely on existing reasoning/knowledge datasets and their officially released unanswerable variants (DeepScaleR, MMLU-Math, GPQA-Diamond, GSM8K, etc.) to avoid injecting extra noise by rewriting prompts.</p>
<p><strong>Overall plan:</strong> Train on stable, previously vetted unanswerable data, then probe generalized refusal behavior on our cleaned cross-domain split to decouple “learning to refuse” from “handling unseen domains.”</p>

<h4>1.2 Experimental Setup & Metrics</h4>
<p>We currently test on the SUM math benchmark (DeepScaleR’s unanswerable extension) and evaluate DeepSeek-R1, Qwen2.5-7B-Instruct, and Qwen2.5-3B-Instruct with four shared metrics:</p>
<ul>
  <li>Accuracy on answerable questions</li>
  <li>False refusals / incorrect “I don’t know” on answerable questions</li>
  <li>Correct refusal rate on unanswerable questions</li>
  <li>Unanswerable questions with a correct refusal <em>and</em> explanation</li>
</ul>

<h4>1.3 Key Findings</h4>
<table>
  <thead>
    <tr>
      <th>Model</th>
      <th>Answerable Accuracy</th>
      <th>Answerable False Refusal</th>
      <th>Unanswerable Correct Refusal</th>
      <th>Unanswerable w/ Correct Explanation</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>DeepSeek-R1</td>
      <td>99.3%</td>
      <td>0.4%</td>
      <td>21.5%</td>
      <td>18.4%</td>
    </tr>
    <tr>
      <td>Qwen2.5-7B-Instruct</td>
      <td>75.4%</td>
      <td>16.9%</td>
      <td>20.4%</td>
      <td>18.5%</td>
    </tr>
    <tr>
      <td>Qwen2.5-3B-Instruct</td>
      <td>43.8%</td>
      <td>6.7%</td>
      <td>15.1%</td>
      <td>13.7%</td>
    </tr>
  </tbody>
  <caption>Table 2. Midterm evaluation on SUM unanswerable split.</caption>
</table>
<p><strong>Compared with SUM baseline numbers:</strong> The SUM paper reports only ~11% refusal rate for Qwen2.5-7B, whereas our prompt + evaluation pipeline yields ~18.5% correct “explain-and-refuse,” showing that enforcing explanation raises the bar beyond binary abstention.</p>
<p><strong>Overall observations:</strong> Model scale and training depth dominate answerable accuracy (3B ≪ 7B &lt; DeepSeek-R1). Even strong reasoning models still refuse only ~20% of unanswerable questions, so boundary awareness remains weak.</p>

<h3>2. Current Issues & Limitations</h3>
<ul>
  <li><strong>LLM-as-Judge noise:</strong> Offline o3-mini decides both “did it refuse?” and “did it cite the real missing condition?” so any judge errors directly add variance to our metrics.</li>
  <li><strong>Evaluation mismatch vs. SUM:</strong> The SUM paper mostly checks for literal <code>I don't know</code> strings, whereas we pass the full response through a judge for semantic scoring, so numbers are not apples-to-apples.</li>
  <li><strong>String matching fails to detect true refusals:</strong> Smaller models often say “I don’t know” and then hallucinate an answer; judge-based semantic checks are required, which reintroduces the noise above.</li>
  <li><strong>Output formatting is underspecified:</strong> Reasoning and answers are interleaved, making auto-evaluation hard; we plan to enforce templates like <code>Reasoning:</code> / <code>Answer:</code> to simplify parsing.</li>
  <li><strong>Cross-domain data cleaning is expensive:</strong> AbstentionBench domains are large and heterogeneous; even with LLM+rule pipelines we still need targeted manual inspection to ensure quality.</li>
</ul>

<hr>
    
<h2 id="results">Results</h2>
<p>
<b>How did you measure success? What experiments were used? What were the results, both quantitative and qualitative? Did you succeed? Did you fail? Why?</b>
</p>
<p>
Nemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit, sed quia consequuntur magni dolores eos qui ratione voluptatem sequi nesciunt.
</p>
<table>
  <thead>
    <tr>
      <th style="text-align: center"><strong>Experiment</strong></th>
      <th style="text-align: center">1</th>
      <th style="text-align: center">2</th>
      <th style="text-align: center">3</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>Sentence</strong></td>
      <td style="text-align: center">Example 1</td>
      <td style="text-align: center">Example 2</td>
      <td style="text-align: center">Example 3</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>Errors</strong></td>
      <td style="text-align: center">error A, error B, error C</td>
      <td style="text-align: center">error C</td>
      <td style="text-align: center">error B</td>
    </tr>
  </tbody>
  <caption>Table 1. This is Table 1's caption</caption>
</table>
<br>
<div style="text-align: center;">
<img style="height: 300px;" alt="" src="./files/results.png">
</div>
<br><br>

<hr>



<h2 id="conclusion">Conclustion and Future Work</h2>
<p>

  How easily are your results able to be reproduced by others?
  Did your dataset or annotation affect other people's choice of research or development projects to undertake?
  Does your work have potential harm or risk to our society? What kinds? If so, how can you address them?
  What limitations does your model have? How can you extend your work for future research?</p>


<hr>


  </div>
  


</body></html>
